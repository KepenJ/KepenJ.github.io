<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Python 爬虫学习（二） | Log'K'</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/4.2.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.0.0/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Python 爬虫学习（二）</h1><a id="logo" href="/.">Log'K'</a><p class="description">Learn more.</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Python 爬虫学习（二）</h1><div class="post-meta">Sep 18, 2015<span> | </span><span class="category"><a href="/categories/Python/">Python</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><div class="post-content"><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在上一篇玩着爬虫学 Python （一）)当中我们针对我们将要用到的 Python 知识点进行了简单的梳理，让我们能有个最基本认识，为我们之后的爬虫学习奠定语言基础。</p>
<h2 id="上网分几步？"><a href="#上网分几步？" class="headerlink" title="上网分几步？"></a>上网分几步？</h2><p>我们的互联网就好比是一张大网，在网上包含着成千上万乃至上亿、兆的信息，而他们分布在世纪各地的服务或电脑中，那我们如何在这个网上获取我们要的信息呢？当然你会想到用 Baidu 、 Yahoo 或者 Google 等等去帮你获取这些东西。然而帮我们的其实是运行在这些公司服务器中的一个个爬虫所作出的贡献，他们就像是一个个不停歇的探索者，不停地去发掘互联网上新的内容并记录、返回给我们，然后头也不回的继续找下去。</p>
<p><img src="https://raw.githubusercontent.com/KepenJ/BlogImages/master/python_2/1.png" alt="1.1"></p>
<p>上面扯了那么多，举个具体的例子来说明好了。假设用户要访问 www.baidu.com 也就是百度的网页，那么他需要如下几步：</p>
<ol>
<li>打开浏览器</li>
<li>在浏览器地址栏输入：www.baidu.com 网址</li>
<li>回车，等待浏览器显示页面</li>
</ol>
<p>ok，其中的技术都集中在了第二步，如果我们从技术的角度来看的话它分为了如下几步：</p>
<ol>
<li>浏览器要访问 www.baidu.com 这个域名，需要先通过 DNS 服务器获取该域名的 IP 地址。</li>
<li>得到 IP 地址后，浏览器和相对应的服务器建立连接。</li>
<li>发送 http/https 请求相对应的 web 页面信息。</li>
<li>服务器响应客户端请求，返回相对应的 web 页面的 html 资源。</li>
<li>浏览器拿到html资源并解析。</li>
<li>显示解析出来的内容。</li>
</ol>
<h2 id="什么是爬虫？"><a href="#什么是爬虫？" class="headerlink" title="什么是爬虫？"></a>什么是爬虫？</h2><p>ok，既然我们知道如何通过网络去获取我们要的资源（参照上面提到的例子）。那么如果我只需要这个页面的某张图片或者链接什么的，而不需要整个页面都显示出来；或者我们要这个网站里面其它页面的更多资源，但我又不想没有页面去点击再去保存…</p>
<p>爬虫就可以帮我们去完成这些。什么是爬虫？爬虫就是一个简单的机器人或者一段程序，用来帮我们去某个网站或者url下去搜需我们要的资源，再按照你要的逻辑或者策略进行操作。更多关于爬虫原理介绍可以参照这篇博文<a href="http://www.cnblogs.com/wawlian/archive/2012/06/18/2553061.html" target="_blank" rel="external">《爬虫的工作原理》</a>。</p>
<h2 id="从一个简单的爬虫去看世界"><a href="#从一个简单的爬虫去看世界" class="headerlink" title="从一个简单的爬虫去看世界"></a>从一个简单的爬虫去看世界</h2><p>talk is cheap，那么作为第一次上手写的爬虫，我们还是直接从源码进行学习。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">__author__ = <span class="string">'KepenJ'</span></div><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="comment">#---------------------------------------</span></div><div class="line"><span class="comment">#   程序：百度贴吧爬虫</span></div><div class="line"><span class="comment">#   版本：0.1</span></div><div class="line"><span class="comment">#   作者：xy</span></div><div class="line"><span class="comment">#   日期：2015-9-17</span></div><div class="line"><span class="comment">#   语言：Python 2.7</span></div><div class="line"><span class="comment">#   操作：输入带分页的地址，去掉最后面的数字，设置一下起始页数和终点页数。</span></div><div class="line"><span class="comment">#   功能：下载对应页码内的所有页面并存储为html文件。</span></div><div class="line"><span class="comment">#---------------------------------------</span></div><div class="line"><span class="keyword">import</span> string,urllib2</div><div class="line"><span class="comment">#定义读写爬虫</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">baidu_tieba</span><span class="params">(begin_page,end_page,url)</span>:</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(begin_page,end_page+<span class="number">1</span>):</div><div class="line">        save_name = string.zfill(i,<span class="number">5</span>) + <span class="string">'.html'</span></div><div class="line">        <span class="keyword">print</span> <span class="string">"正在下载第 %d 页的帖子信息..."</span> % (i)</div><div class="line">        f = open(save_name,<span class="string">'w+'</span>)</div><div class="line">        reqest = urllib2.Request(url+<span class="string">'?'</span>+<span class="string">'pn='</span>+str(i))</div><div class="line">        m = urllib2.urlopen(reqest).read()</div><div class="line">        f.write(m)</div><div class="line">        f.close()</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">print</span> <span class="string">"下载完成！"</span></div><div class="line">request_url = str(<span class="string">u'http://tieba.baidu.com/p/4041128662'</span>)</div><div class="line">begin_num = int(raw_input(<span class="string">"起始："</span>))</div><div class="line">end_num = int(raw_input(<span class="string">"结束："</span>))</div><div class="line">baidu_tieba(begin_page=begin_num,end_page=end_num,url=request_url)</div></pre></td></tr></table></figure>
<p>上面就是我们第一个爬虫的源码，这是一个爬百度某贴吧的页面并保存本地的爬虫，他大概的设计思路如下：</p>
<ul>
<li>抓取某贴吧 url 的 html 内容。</li>
<li>支持用户输入抓取的起始位置和结束位置。</li>
<li>抓取到的页面信息保存到本地。</li>
</ul>
<p>那么我们就来拆分一下这个源码，首先是所有 python 文件的头信息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">__author__ = &apos;KepenJ&apos;</div></pre></td></tr></table></figure>
<p>关键字 <code>__author__</code> 用来声明该头文件的作者信息。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># -*- coding: utf-8 -*-</div></pre></td></tr></table></figure>
<p>在上一篇 <a href="http://kepenj.me/blog/2015/09/16/wan-zhao-pa-chong-xue-python/\(ji-chu-pian-/" target="_blank" rel="external">玩着爬虫学 Python（一)</a> 中我们有提到过，用来声明该文件的编码方式采用 UTF-8。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">import string,urllib2</div></pre></td></tr></table></figure>
<p>接下来我们就要需要导入 <code>string</code> 用来做字符串相关拼接和处理。 <code>urllib2</code> 包则用来尽心网络信息的获取，也就是我们用来访问网络和处理相关请求结果的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#定义读写爬虫</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">baidu_tieba</span><span class="params">(begin_page,end_page,url)</span>:</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(begin_page,end_page+<span class="number">1</span>):</div><div class="line">        save_name = string.zfill(i,<span class="number">5</span>) + <span class="string">'.html'</span></div><div class="line">        <span class="keyword">print</span> <span class="string">"正在下载第 %d 页的帖子信息..."</span> % (i)</div><div class="line">        f = open(save_name,<span class="string">'w+'</span>)</div><div class="line">        reqest = urllib2.Request(url+<span class="string">'?'</span>+<span class="string">'pn='</span>+str(i))</div><div class="line">        m = urllib2.urlopen(reqest).read()</div><div class="line">        f.write(m)</div><div class="line">        f.close()</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">print</span> <span class="string">"下载完成！"</span></div></pre></td></tr></table></figure>
<p>下来就是我们的核心，一个读写功能的函数。它构成了整个爬虫的核心。这个函数分为以下几步：</p>
<ol>
<li>外部传入 begin_page 、 end_page 和 url ，也就是我们的起始、结束的页数和要抓取的链接地址。</li>
<li>通过 range() 函数生成一个 list 用来遍历用。同时通过 for 循环来遍历。</li>
<li>接下来就是通过 string.zfill() 和 .html 构建抓取后用来存储的文件名称 save_name</li>
<li>open() 创建可读( w+ )的文件，名称就是之前的 save_name，并打开。</li>
<li>使用 urllib2 包中的 Request() 函数来构建一个请求。</li>
<li>在通过 urllib2 中的 urlopen() 函数来请求内容，并用 read() 来获取亲求下来的内容。</li>
<li>将内容通过 write() 函数写入文件中。</li>
<li>关闭文件。</li>
<li>循环 2～8 直至结束，然后 else 打印结果。</li>
</ol>
<p>以上就是我们的核心功能，接下来只需要去调用这个函数就能完成我们整个爬虫。同样的我们来结束这个源码的说明。</p>
<p>构建需要抓取的 url 链接</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">request_url = str(u&apos;http://tieba.baidu.com/p/4041128662&apos;)</div></pre></td></tr></table></figure>
<p>通过 <code>raw_input()</code> 函数来接收用户所输入的起始和结束值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">begin_num = int(raw_input(<span class="string">"起始："</span>))</div><div class="line">end_num = int(raw_input(<span class="string">"结束："</span>))</div></pre></td></tr></table></figure>
<p>调用之前写好的 <code>baidu_tieba()</code> 来实现我们整爬虫的功能。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">baidu_tieba(begin_page=begin_num,end_page=end_num,url=request_url)</div></pre></td></tr></table></figure>
<p>最后附 2 张 Run 后的结果图：</p>
<p><img src="https://raw.githubusercontent.com/KepenJ/BlogImages/master/python_2/2.png" alt="1.2"></p>
<p><img src="https://raw.githubusercontent.com/KepenJ/BlogImages/master/python_2/3.png" alt="1.3"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过上面这个简答的爬虫，才正式开启了之后的爬虫之旅。之后还需要我们学会如何去处理这些抓下来的 html 内容，从中获取我们需要的结果，我们还要如何讲究效率的去获取这些。更多的更多我们都应该去深入学习。</p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://yoursite.com/2015/09/18/Python-爬虫学习（二）/" data-id="cis7j69kd000frpu5zkggsv26" class="article-share-link">分享到</a><div class="tags"></div><div class="post-nav"><a href="/2015/09/28/Python-爬虫学习（三）/" class="pre">Python 爬虫学习（三）</a><a href="/2015/09/16/Python-爬虫学习（一）/" class="next">Python 爬虫学习（一）</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/C/">C++</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo/">Hexo</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/iOS/">iOS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/杂感/">杂感</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2016/05/21/C-学习笔记（二）：类和对象/">C++学习笔记（二）：类和对象</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/04/27/C-学习笔记（一）：C-基本和新增特性/">C++学习笔记（一）：C++基本和新增特性</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/04/21/iAP-的一波玩弄/">iAP 的一波玩弄</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/04/20/A-new-beginning/">A new beginning.</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/03/28/关于近期面试有感/">关于近期面试有感</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/03/01/记一次失败的OS-X-重装/">记一次失败的OS X 重装</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/02/29/从-Octopress-换-Hexo-了/">从 Octopress 换 Hexo 了</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/02/21/Apple-pay-体验报告/">Apple pay 体验报告</a></li><li class="post-list-item"><a class="post-list-link" href="/2015/12/29/2015-终总结和-2016-的-flag/">2015 终总结和 2016 的 flag</a></li><li class="post-list-item"><a class="post-list-link" href="/2015/10/22/Scrapy-爬虫学习中不可或缺的利器/">Scrapy 爬虫学习中不可或缺的利器</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://github.com/KepenJ" title="Github" target="_blank">Github</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">Log'K'.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>